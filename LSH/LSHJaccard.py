import sys
from collections import defaultdict
import collections
from pyspark import SparkContext
from pyspark.sql import *
import itertools
import time

if(sys.argv.__len__() >= 1):
    fileInput = str(sys.argv[1])
else:
    fileInput = "ratings.csv"

# Starting the timer
startTimer = time.time()

# Initiaing the sparkcontext to create RDD for ratings and movies
sc = SparkContext("local[*]", "LSH Implementation using Jaccard Similarity")

# Reading the ratings.csv file in rantingsInput RDD
ratingsInput = sc.textFile(fileInput)

# Get the row data of ratings.csv
heading = ratingsInput.first()
rows = ratingsInput.filter(lambda row: row != heading)

# Creating a map with key value pair for all the movies rated by the user
mergeUserRatingsToMovie = rows.map(lambda line: line.split(",")) \
    .map(lambda line: (int(line[1]), [int(line[0])])) \
    .reduceByKey(lambda x, y: x + y)

# Created a list of each movie rated by all the user
eachMovieUserList = mergeUserRatingsToMovie.collect()

# hash count value which will be used for clustering
hashCount = 200

# Function to create a signature matrix for the movies using hash function


def createSignMatrix(eachLine):
    returnValue = []
    for eachHashFunction in range(hashCount):
        presentValueInColumns = float('Inf')
        for eachUser in eachLine[1]:
            presentValueInColumns = min(presentValueInColumns, ((
                # numbers are random to make an ideal hash function
                (15+eachHashFunction) * eachUser + (34+eachHashFunction)) % 717))
        returnValue.append(str(presentValueInColumns))
    return (eachLine[0], returnValue)


# Signature matrix with hash function on data
signatureMatrix = mergeUserRatingsToMovie.map(createSignMatrix)

totalBands = 40
eachBandLength = hashCount / totalBands
eachBandValue = 0
pairsCandidate = sc.emptyRDD()
bucket_movieIds = defaultdict(lambda: 0)

# Candidate pairs are generated by checking two similar movies based on bands


def getCandidatePairs(inputval):
    eachMovieUserList = inputval
    total = 0
    for each in eachMovieUserList[1]:
        total = total + int(each)
    total = total % 201

    return (total, inputval)


totalCandPairs = list()
totalCandDict = defaultdict(lambda: 0)

# Calculating the final codidates based matrix data


def calculateFinal(eachLine):
    global totalCandDict
    eachMovieUserList = list(eachLine[1])
    # get all movie combinations of size 2
    moviesCombinations = itertools.combinations(eachMovieUserList, 2)
    count = 0
    for eachComb in moviesCombinations:
        if(eachComb[0][1] == eachComb[1][1]) and totalCandDict[tuple(sorted([eachComb[0][0], eachComb[1][0]]))] == 0:
            count += 1
            totalCandDict[tuple(sorted([eachComb[0][0], eachComb[1][0]]))] = 1


finalSet = set()

# Checking each band in the range and create the candidate pairs
for eachBand in range(totalBands):
    eachBandRDD = signatureMatrix.map(lambda x: (
        x[0], x[1][int(eachBandValue): int(eachBandValue + eachBandLength)]))
    candidatePairsOutput = eachBandRDD.map(getCandidatePairs)
    moviesAddedInBuckets = candidatePairsOutput.combineByKey(
        lambda x: [x], lambda x, y: x + [y], lambda x, y: x + y)
    finalCandidatePairs = moviesAddedInBuckets.collect()
    for eachline in finalCandidatePairs:
        calculateFinal(eachline)
    eachBandValue = eachBandValue + eachBandLength

tempDict = dict()

for eachMovie in eachMovieUserList:
    tempDict[eachMovie[0]] = eachMovie[1]

finalOutput = dict()

for each in totalCandDict:
    userList1 = set(tempDict[each[0]])
    userList2 = set(tempDict[each[1]])
    # Jaccard Similarity calculation by intersecting (userList1, userList2) and dividing them by their union
    num = len(userList1 & userList2) / float(len(userList1 | userList2))
    # Check if the similarity is greater than 0.5
    if (num >= 0.5):
        finalOutput[each] = num

print(len(finalOutput))

endTimer = time.time()
# Calculating the total time taken
totalTime = endTimer-startTimer

# writeTofile = 'SimilarityReport.txt'
fileOutputWrite = open('SimilarityReport.txt', 'w')
# output data is sorted based on movie id
finalSetToWrite = collections.OrderedDict(sorted(finalOutput.items()))

# Reading movies.csv file in order to retrive the movie title for specified movie id
spark = SparkSession.builder.appName(
    'Read CSV File into DataFrame').getOrCreate()

movieData = spark.read.option('header', True).csv('movies.csv')

# Output the results in the file
for m, s in finalSetToWrite.items():
    movie1 = movieData.filter('movieId = ' + str(m[0])).collect()
    movie2 = movieData.filter('movieId = ' + str(m[1])).collect()
    # Out the candidate pairs with their similarity
    fileOutputWrite.write(
        "Movie 1 -> [id:" + str(m[0]) + ', name:' + movie1[0].__getitem__('title')+'], Movie 2 -> [id:' + str(m[1]) + ', name:' + movie2[0].__getitem__('title') + '], Similarity = ' + str(s))
    fileOutputWrite.write("\n")

# Printing the toral time taken by the implementation
print('Total time taken = ' + str(totalTime))
